Given a URL `startUrl` and an interface `HtmlParser`, implement **a Multi-threaded web crawler** to crawl all links that are under the **same hostname** as `startUrl`.

Return all URLs obtained by your web crawler in **any** order.

Your crawler should:

- Start from the page: `startUrl`
- Call `HtmlParser.getUrls(url)` to get all URLs from a webpage of a given URL.
- Do not crawl the same link twice.
- Explore only the links that are under the **same hostname** as `startUrl`.


```java
/**
 * // This is the HtmlParser's API interface.
 * // You should not implement it, or speculate about its implementation
 * interface HtmlParser {
 *     public List<String> getUrls(String url) {}
 * }
 */
class Solution {

    class Crawler implements Runnable {
        String url;
        String hostName;
        HtmlParser htmlParser;
        public static Set<String> result;

        public Crawler(String url, String hostName, HtmlParser htmlParser) {
            this.url = url;
            this.hostName = hostName;
            this.htmlParser = htmlParser;
        }

        @Override
        public void run() {
            if(url.startsWith(hostName) && !result.contains(url)) {
                result.add(url);

                List<Thread> threads = new ArrayList<>();

                for(String neighUrl : htmlParser.getUrls(url)) {
                    Crawler crawler = new Crawler(neighUrl, hostName, htmlParser);
                    Thread thread = new Thread(crawler);
                    thread.start();

                    threads.add(thread);
                }

                for(Thread t : threads) {
                    joinThread(t);
                }
            }
        }

        public static void joinThread(Thread t) {
            try {
                t.join();
            } catch(Exception e) {
                e.printStackTrace();
            }
        }
    }

    public List<String> crawl(String startUrl, HtmlParser htmlParser) {
        int index = startUrl.indexOf('/', 7);
        String hostName = (index == -1) ? startUrl : startUrl.substring(0, index);
        Crawler crawler = new Crawler(startUrl, hostName, htmlParser);
        
        ConcurrentHashMap<String, Boolean> map = new ConcurrentHashMap<>();
        crawler.result = map.newKeySet();

        Thread thread = new Thread(crawler);
        
        thread.start();

        crawler.joinThread(thread);

        return new ArrayList<>(crawler.result);
    }
}
```

**Follow up:**

1. Assume we have 10,000 nodes and 1 billion URLs to crawl. We will deploy the same software onto each node. The software can know about all the nodes. We have to minimize communication between machines and make sure each node does equal amount of work. How would your web crawler design change?
2. What if one node fails or does not work?
3. How do you know when the crawler is done?


```java
/**
 * // This is the HtmlParser's API interface.
 * // You should not implement it, or speculate about its implementation
 * interface HtmlParser {
 *     public List<String> getUrls(String url) {}
 * }
 */
class Solution {

    class Crawler implements Runnable {
        String url;
        String hostName;
        HtmlParser htmlParser;
        public static Set<String> result;
        public static ExecutorService pool;

        public Crawler(String url, String hostName, HtmlParser htmlParser) {
            this.url = url;
            this.hostName = hostName;
            this.htmlParser = htmlParser;
        }

        @Override
        public void run() {
            if(url.startsWith(hostName) && !result.contains(url)) {
                result.add(url);

                List<Future<?>> tasks = new LinkedList<>();

                for(String neighUrl : htmlParser.getUrls(url)) {
                    Crawler crawler = new Crawler(neighUrl, hostName, htmlParser);
                    // Thread thread = new Thread(crawler);
                    // thread.start();

                    tasks.add(pool.submit(crawler));
                }

                for(Future<?> t : tasks) {
                    joinThread(t);
                }
            }
        }

        public static void joinThread(Future<?> t) {
            try {
                t.get();
            } catch(Exception e) {
                e.printStackTrace();
            }
        }
    }

    public List<String> crawl(String startUrl, HtmlParser htmlParser) {
        int index = startUrl.indexOf('/', 7);
        String hostName = (index == -1) ? startUrl : startUrl.substring(0, index);
        Crawler crawler = new Crawler(startUrl, hostName, htmlParser);
        
        // ConcurrentHashMap<String, Boolean> map = new ConcurrentHashMap<>();
        ExecutorService pool = Executors.newFixedThreadPool(500);
        crawler.result = ConcurrentHashMap.newKeySet();
        crawler.pool = pool;

        // Thread thread = new Thread(crawler);
        // thread.start();

        Future<?> task = pool.submit(crawler);

        crawler.joinThread(task);
        pool.shutdown();

        return new ArrayList<>(crawler.result);
    }
}
```




